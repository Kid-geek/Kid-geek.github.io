<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[阿里云配置MySQL]]></title>
      <url>%2F2018%2F01%2F30%2F%E9%98%BF%E9%87%8C%E4%BA%91%E9%85%8D%E7%BD%AEMySQL%2F</url>
      <content type="text"><![CDATA[安装MySQL先查看是否已经安装rpm -qa|grep -i mysql mysql的repo源# wget http://repo.mysql.com//mysql57-community-release-el7-7.noarch.rpm# rpm -ivh mysql57-community-release-el7-7.noarch.rpm安装之后会获得/etc/yum.repos.d/mysql-community.repo和/etc/yum.repos.d/mysql-community-source.repo两个repo源 yum安装# yum install mysql-server# yum install mysql-devel 启动MySQL# service mysqld start启动mysql# service mysqld status查看mysql当前的状态# service mysqld stop停止mysql# service mysqld restart重启mysql连接MySQL 首次登陆需要先查看默认密码 # cat /var/log/mysqld.log | grep password 登录MySQLmysql -u root -h localhost -p 输入密码 首次操作要先修改密码,但默认安全级别为最高,需手动调低,不然设置密码会很容易提示太简单而修改失败 set global validate_password_policy=0; 修改密码 set PASSWORD = PASSWORD(&#39;123456&#39;); 配置远程连接MySQL 首先配置阿里云安全组规则,允许开放3306接口 登录MySQL可以设置对所有IP开放update user set host=&#39;%&#39; where user=&#39;root&#39; and host=&#39;localhost&#39;;也可以将host指定为某个ipupdate user set host=&#39;106.39.178.131&#39; where user=&#39;root&#39; and host=&#39;localhost&#39;; 刷新权限表,使配置生效 flush privileges; 至此即可远程连接MYSQL 参考链接:阿里云下配置MySQL远程连接]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch查询命令]]></title>
      <url>%2F2018%2F01%2F30%2FElasticsearch%E6%9F%A5%E8%AF%A2%E5%91%BD%E4%BB%A4%2F</url>
      <content type="text"><![CDATA[添加映射 12345678910111213141516171819202122232425262728PUT lagou &#123; &quot;mappings&quot;: &#123; &quot;job&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;store&quot;:true, &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;company_name&quot;:&#123; &quot;store&quot;: true, &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;desc&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;comments&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;add_time&quot;:&#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot; &#125; &#125; &#125; &#125; &#125; 添加数据123456789101112131415161718192021222324252627POST lagou/job/&#123; &quot;title&quot;:&quot;python djiango开发&quot;, &quot;companya_name&quot;:&quot;美团&quot;, &quot;desc&quot;:&quot;对Django的概念熟悉,熟悉Python&quot;, &quot;comments&quot;:20, &quot;add_time&quot;:&quot;2017-4-1&quot;&#125;POST lagou/job/&#123; &quot;title&quot;:&quot;python开发&quot;, &quot;companya_name&quot;:&quot;美团&quot;, &quot;desc&quot;:&quot;对Django的概念熟悉,熟悉Python&quot;, &quot;comments&quot;:20, &quot;add_time&quot;:&quot;2017-5-1&quot;&#125;POST lagou/job/&#123; &quot;title&quot;:&quot;python 分布式爬虫开发&quot;, &quot;companya_name&quot;:&quot;百度科技&quot;, &quot;desc&quot;:&quot;对scrapy的概念熟悉,熟悉Python&quot;, &quot;comments&quot;:50, &quot;add_time&quot;:&quot;2017-4-1&quot;&#125; 查询命令match查询 分词查询12345678GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;爬虫开发&quot; &#125; &#125;&#125; term查询 全量查询12345678GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;python&quot; &#125; &#125;&#125; terms查询1234567891011GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;title&quot;: [ &quot;爬虫&quot;, &quot;开发&quot; ] &#125; &#125;&#125; match查询 分词查询 控制查询的返回数量12345678910GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;开发&quot; &#125; &#125;, &quot;from&quot;: 0, &quot;size&quot;: 3&#125; match_all查询123456GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; match_phrase查询 要满足所有词才返回结果1234567891011GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;python开发&quot;, &quot;slop&quot;:10 &#125; &#125; &#125;&#125; multi_match 查询 fields中有满足即可123456789GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;熟悉&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;desc&quot;] &#125; &#125;&#125; 返回store为true的字段123456789GET lagou/_search&#123; &quot;stored_fields&quot;: [&quot;title&quot;,&quot;company_name&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;python&quot; &#125; &#125;&#125; 排序12345678910111213GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;comments&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 查询范围12345678910111213141516171819202122232425GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;comments&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 50, &quot;boost&quot;: 2.0 &#125; &#125; &#125;&#125;GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;add_time&quot;: &#123; &quot;gte&quot;: &quot;2017-04-01&quot;, &quot;lte&quot;: &quot;now&quot;, &quot;boost&quot;: 2.0 &#125; &#125; &#125;&#125; wildcard查询 通配符模糊查询12345678910GET lagou/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;dj*o&quot; &#125; &#125; &#125;&#125; Bool查询1234567#bool包括 must shold must_not filter#格式:#bool:&#123;#&quot;filter&quot;:[],#&quot;must&quot;:[],#&quot;shold&quot;:[],#&quot;must_not&quot;:[], 插入数据123456789POST lagou/testjob/_bulk&#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;&#123;&quot;salary&quot;:10,&quot;title&quot;:&quot;Python&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;&#123;&quot;salary&quot;:20,&quot;title&quot;:&quot;Scrapy&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;&#123;&quot;salary&quot;:30,&quot;title&quot;:&quot;Django&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:4&#125;&#125;&#123;&quot;salary&quot;:30,&quot;title&quot;:&quot;Elasticsearch&quot;&#125; 薪资20K123456789101112131415GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match_all&quot;: &#123;&#125;&#125; ], &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;salary&quot;: &quot;30&quot; &#125; &#125; &#125; &#125; &#125; 指定多个值1234567891011121314GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match_all&quot;: &#123;&#125;&#125; ] , &quot;filter&quot;: &#123;&quot;terms&quot;: &#123; &quot;salary&quot;: [10,30] &#125;&#125; &#125; &#125;&#125; select * from testjob where title=”Python”12345678910111213GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match_all&quot;: &#123;&#125;&#125; ], &quot;filter&quot;: &#123;&quot;match&quot;: &#123;&quot;title&quot;:&quot;Python&quot;&#125; &#125; &#125; &#125;&#125; 查看分析结果12345GET _analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;:&quot;Python开发工程师&quot;&#125; select * from testjob where(salary=20 OR title=Pyhon) AND (salary!=30)1234567891011121314GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;:&#123;&quot;salary&quot;:20&#125;&#125;, &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;python&quot;&#125;&#125; ], &quot;must_not&quot;: [ &#123;&quot;term&quot;:&#123;&quot;salary&quot;:30&#125;&#125; ] &#125; &#125;&#125; 嵌套查询select * from testjob where title=&quot;python&quot; or (title=&quot;django&quot; AND salary=30)123456789101112131415161718GET lagou/testjob/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123;&quot;term&quot;: &#123; &quot;title&quot;:&quot;python&quot; &#125;&#125;, &#123;&quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;django&quot;&#125;&#125;, &#123;&quot;term&quot;:&#123;&quot;salary&quot;:30&#125;&#125; ] &#125;&#125; ] &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Scrapy随机更换User-Agent]]></title>
      <url>%2F2018%2F01%2F29%2FScrapy%E9%9A%8F%E6%9C%BA%E6%9B%B4%E6%8D%A2User-Agent%2F</url>
      <content type="text"><![CDATA[利用DownMIDLEWARE实现UA随机更换安装fake-useragent库,随机生成UApip install fake-useragent github地址 在middlewares创建类123456789101112131415from fake_useragent import UserAgent#随机更换UserAgentclass RandomUserAgentMiddleware(object): def __init__(self,crawler): super(RandomUserAgentMiddleware,self).__init__() @classmethod def from_crawler(cls,crawler): return cls(crawler) def process_request(self,request,spider): # 调用fake-useragent 随机生成UA ua = UserAgent() request.headers.setdefault('User-Agent',ua.random) 在settings中设置 DOWNLOADER_MIDDLEWARES1234DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'lagou.middlewares.RandomUserAgentMiddleware': 500,&#125; 注意:要先取消掉Scrapy默认设置UA的Middleware]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建Elasticsearch]]></title>
      <url>%2F2018%2F01%2F26%2F%E6%90%AD%E5%BB%BAElasticsearch%2F</url>
      <content type="text"><![CDATA[搭建JAVA环境 地址 安装elasticsearch-rtf (国人制作安装好插件的elasticsearch) 地址直接从github搜索,下完后解压即可 在bin目录下直接用命令行运行elasticsearch即可,端口号9200 搭建NodeJS环境 地址 安装head插件进行管理 地址直接从github搜索,下完解压缩先安装cnpm npm install -g cnpm --registry=https://registry.npm.taobao.org然后 在head目录下命令行输入 cnpm install cnpm run start 即可, 端口号9100 安装kibana 在官网下载对应版本为 5.1.1 地址下完后解压即可 在bin目录下直接用命令行运行kibana.bat即可 端口号5601]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linxu的文件权限和目录配置]]></title>
      <url>%2F2018%2F01%2F17%2FLinxu%E7%9A%84%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E5%92%8C%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[读书笔记 Linux文件权限和目录配置Linux文件属性ls -l命令查看详细内容-rw-r--r-- 1 root root 1023 Sep 4 18:25 test.txt 分别为 [权限] [连接] [所有者] [用户组] [文件容量] [修 改 日 期] [文件名] 权限: 第一个字符代表这个文件是”目录、文件或链接文件等”· [d] : 目录· [-] : 文件· [i] : 连接文件· [b] : 设备文件里面的可供存储的接口设备· [c] : 表示设备文件里面的串行端口设备, 例如键盘鼠标等 权限与命令间的关系 让用户能进入某目录成为’可工作目录’的基本权限是什么· 可使用的命令: 例如cd等切换工作目录的命令· 目录所需权限: 用户对这个目录至少需要 x 的权限· 额外需求: 如果用户想用ls查阅,则用户对此目录还需有 r 的权限 用户在某个目录内读取一个文件的基本权限是什么· 可使用的命令: 例如 cat more less等· 目录所需权限: 用户对这个目录至少需要具有 x 权限· 文件所需权限: 用户对文件至少需要 r 的权限 让用户修改一个文件的基本权限是什么· 可使用的命令: 例如 nano 或 vi 等· 目录所需权限: 用户对这个目录至少需要具有 x 权限· 文件所需权限: 用户对文件至少需要 r,w 的权限 让一个用户可以创建一个文件的基本权限是什么· 目录所需权限: 用户在该目录至少具有 w x 的权限 重点在w 让用户进入某目录并执行该目录下某个命令的基本权限是什么· 目录所需权限: 用户对这个目录至少需要具有 x 权限· 文件所需权限: 用户对文件至少需要 x 的权限]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阿里云配置Anaconda]]></title>
      <url>%2F2018%2F01%2F16%2F%E9%98%BF%E9%87%8C%E4%BA%91%E9%85%8D%E7%BD%AEAnaconda%2F</url>
      <content type="text"><![CDATA[安装Anaconda官网下载Anaconda-Linux版本 yum install bzip2安装依赖 (很重要) bash anaconda.sh 安装Anaconda 一路回车之后选择yes yes yes source ~/.bashrc 激活命令 python测试安装是否成功 手动添加PATH变量.echo &#39;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阿里云配置Redis远程连接]]></title>
      <url>%2F2018%2F01%2F16%2F%E9%98%BF%E9%87%8C%E4%BA%91%E9%85%8D%E7%BD%AERedis%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%2F</url>
      <content type="text"><![CDATA[为了配置Scrapy-redis分布式爬虫, 遇到超多坑, 记录一下 安装Redisyum -y install redis 直接输入命令即可 修改配置文件vim /etc/redis.conf bind 127.0.0.1 代表指定Redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，在生产环境中最好设置该项。如果需要远程连接 则注释掉该项即可 daemonize yes 代表是否后台运行 在redis3.2之后，redis增加了protected-mode，在这个模式下，即使注释掉了bind 127.0.0.1，再访问redisd时候还是报错,解决办法:protected-mode yes 修改为 protected-mode no requirepass xxxx xxxx代表连接Redis密码 远程连接Reidsredis-cli -h ip -a password Ip填写阿里云外网IP 密码填写密码即可]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[同时运行多个Scrapy爬虫的方法]]></title>
      <url>%2F2018%2F01%2F12%2F%E5%90%8C%E6%97%B6%E8%BF%90%E8%A1%8C%E5%A4%9A%E4%B8%AAScrapy%E7%88%AC%E8%99%AB%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[一个项目有时有多个爬虫, 可以自定义项目名录达到一次运行多个爬虫的目的. scrapy list可以查看当前项目下有几个爬虫 创建commands目录在项目根目录下创建commands目录 创建crawlall.py文件代码如下: 123456789101112131415161718192021222324252627282930313233343536373839from scrapy.commands import ScrapyCommand from scrapy.crawler import CrawlerRunnerfrom scrapy.utils.conf import arglist_to_dictclass Command(ScrapyCommand): requires_project = True def syntax(self): return '[options]' def short_desc(self): return 'Runs all of the spiders' def add_options(self, parser): ScrapyCommand.add_options(self, parser) parser.add_option("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE", help="set spider argument (may be repeated)") parser.add_option("-o", "--output", metavar="FILE", help="dump scraped items into FILE (use - for stdout)") parser.add_option("-t", "--output-format", metavar="FORMAT", help="format to use for dumping items with -o") def process_options(self, args, opts): ScrapyCommand.process_options(self, args, opts) try: opts.spargs = arglist_to_dict(opts.spargs) except ValueError: raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False) def run(self, args, opts): #settings = get_project_settings() spider_loader = self.crawler_process.spider_loader for spidername in args or spider_loader.list(): print "*********cralall spidername************" + spidername self.crawler_process.crawl(spidername, **opts.spargs) self.crawler_process.start() 这里主要是用了self.crawler_process.spider_loader.list()方法获取项目下所有的spider，然后利用self.crawler_process.crawl运行spider 创建__init__.py文件 在settings.py中添加配置:COMMANDS_MODULE = &#39;cnblogs.commands&#39; 在命令行中输入scrapy crawlall即可 参考自:同时运行多个scrapy爬虫的几种方法（自定义scrapy项目命令）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Scrapy配置MySQL]]></title>
      <url>%2F2018%2F01%2F12%2FScrapy%E9%85%8D%E7%BD%AEMySQL%2F</url>
      <content type="text"><![CDATA[配置Item导入MySQL同步插入数据 1234567891011121314151617181920212223242526272829303132333435import MySQLdb.cursorsfrom scrapy.utils.project import get_project_settings# 同步MYSQLclass MySQLPipeline(object): def open_spider(self,spider): setting=get_project_settings() db = setting.get('MYSQL_DB_NAME', 'lagou_scrapy') host = setting.get('MYSQL_HOST', 'localhost') port = setting.get('MYSQL_PORT', 3306) user = setting.get('MYSQL_USER', 'root') passwd = setting.get('MYSQL_PASSWORD', '123456') self.db_conn=MySQLdb.connect(host=host,port=port,db=db,user=user,passwd=passwd,charset='utf8') self.db_cur=self.db_conn.cursor() def close_spider(self,spider): self.db_conn.commit() self.db_conn.close() def process_item(self,item,spider): self.insert_db(item) return item def insert_db(self,itme): values=(itme['positionName'], itme['companyShortName'], itme['salary'], itme['positionAdvantage'], ) sql='INSERT INTO java_beijing (positionName, companyShortName, salary, positionAdvantage) VALUES ("%s", "%s", "%s", "%s")' self.db_cur.execute(sql,values) 异步插入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from twisted.enterprise import adbapiimport MySQLdb.cursorsfrom scrapy.utils.project import get_project_settings# 异步 MYSQLclass MySQLAsyncPipeline(object): def __init__(self): setting=get_project_settings() db = setting.get('MYSQL_DB_NAME', 'lagou_scrapy') host = setting.get('MYSQL_HOST', 'localhost') port = setting.get('MYSQL_PORT', 3306) user = setting.get('MYSQL_USER', 'root') passwd = setting.get('MYSQL_PASSWORD', '123456') self.dbpool=adbapi.ConnectionPool('MySQLdb',host=host,db=db,user=user,passwd=passwd,charset='utf8') def close_spider(self,spider): self.dbpool.close() def process_item(self,item,spider): self.dbpool.runInteraction(self.insert_db,item) return item def insert_db(self,tx,itme): values=(itme['positionName'], itme['companyShortName'], itme['salary'], itme['industryField'], itme['positionAdvantage'], itme['workYear'], itme['education'], itme['jobNature'], itme['positionId'], itme['createTime'], itme['city'], itme['district'], itme['companyFullName'], itme['financeStage'], itme['companySize'], itme['info'] ) sql='INSERT IGNORE INTO java_beijing (positionName, companyShortName, salary, industryField, positionAdvantage, workYear, education, jobNature, positionId, createTime, city, district, companyFullName, financeStage, companySize, info) VALUES ("%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s", "%s")' tx.execute(sql,values)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python发送邮件]]></title>
      <url>%2F2018%2F01%2F12%2FPython%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[考虑到后期可以做自动发送邮件提醒, 记录一下Python发送邮件 开启QQ邮箱的设置-账户里SMTP服务,会得到一个授权码 123456789101112131415161718192021# coding=utf-8import smtplibfrom email.mime.text import MIMETextmsg_from = '280705132@qq.com' # 发送方邮箱passwd = ' ' # 填入发送方邮箱的授权码msg_to = '280705132@qq.com' # 收件人邮箱subject = "python邮件测试" # 主题content = "这是我使用python smtplib及email模块发送的邮件"msg = MIMEText(content)msg['Subject'] = subjectmsg['From'] = msg_frommsg['To'] = msg_totry: s = smtplib.SMTP_SSL("smtp.qq.com", 465) s.login(msg_from, passwd) s.sendmail(msg_from, msg_to, msg.as_string()) print("发送成功")except smtplib.SMTPException: print("发送失败")]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Scrapy拉勾]]></title>
      <url>%2F2018%2F01%2F12%2FScrapy%E6%8B%89%E5%8B%BE%2F</url>
      <content type="text"><![CDATA[爬取拉勾网招聘信息抓列表后台查看到内容是AJAX生成的,并且POST不同URL获取到不同内容, 但表单参数只作为控制页码使用.参数:fd = {&#39;first&#39;: &#39;false&#39;, &#39;pn&#39;: 1, &#39;kd&#39;: &#39;java&#39;}其中 pn控制页码 拉勾写死在最多显示30页内容 每页15条 kd控制职位关键字 输入要搜索的职位名称即可 POST URL规则如下: px 排序方式: new 最新 default 默认gx 工作性质: 全职 实习gj 工作经验: 不限 应届毕业生 3年及以下 3-5年 5-10年 10年以上 不要求city 城市: 北京 例如:用此规则构建 工作经验:3年及以下 排序方式: 最新 城市: 北京得到URL:https://www.lagou.com/jobs/positionAjax.json?gj=3年及以下&amp;px=new&amp;city=北京&amp;needAddtionalResult=false&amp;isSchoolJob=0 构造完URL后, POST请求后得到JSON格式字符串, 转换为字典格式解析即可, 根据自己需求保存信息其中注意 positionId参数 为职位ID, 根据此ID构造职位详情URL 注1: 在POST请求时 反爬验证为Referer 设置头为 &#39;Referer&#39;:&#39;https://www.lagou.com/jobs/list_java?px=new&amp;city=%E5%8C%97%E4%BA%AC&#39;注2: 当工作经验为应届毕业生时,URL特为https://www.lagou.com/jobs/positionAjax.json?px=default&amp;gx=全职&amp;city=北京&amp;needAddtionalResult=false&amp;isSchoolJob=1 抓详情通过positionId参数,构造详情URL:https://www.lagou.com/jobs/+ positionId然后GET请求此参数即可. 注: 当大规模抓取时, 速度过快会导致IP被查到, 结果为 302请求跳转到登录页面. 通过Google解决办法在 spider.py文件中 添加默认请求头即可.123456789101112131415custom_settings = &#123; "COOKIES_ENABLED": False, # "DOWNLOAD_DELAY": 1, 'DEFAULT_REQUEST_HEADERS': &#123; 'Accept': 'application/json, text/javascript, */*; q=0.01', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Connection': 'keep-alive', 'Cookie': 'user_trace_token=20171015132411-12af3b52-3a51-466f-bfae-a98fc96b4f90; LGUID=20171015132412-13eaf40f-b169-11e7-960b-525400f775ce; SEARCH_ID=070e82cdbbc04cc8b97710c2c0159ce1; ab_test_random_num=0; X_HTTP_TOKEN=d1cf855aacf760c3965ee017e0d3eb96; showExpriedIndex=1; showExpriedCompanyHome=1; showExpriedMyPublish=1; hasDeliver=0; PRE_UTM=; PRE_HOST=www.baidu.com; PRE_SITE=https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DsXIrWUxpNGLE2g_bKzlUCXPTRJMHxfCs6L20RqgCpUq%26wd%3D%26eqid%3Dee53adaf00026e940000000559e354cc; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2F; index_location_city=%E5%85%A8%E5%9B%BD; TG-TRACK-CODE=index_hotjob; login=false; unick=""; _putrc=""; JSESSIONID=ABAAABAAAFCAAEG50060B788C4EED616EB9D1BF30380575; _gat=1; _ga=GA1.2.471681568.1508045060; LGSID=20171015203008-94e1afa5-b1a4-11e7-9788-525400f775ce; LGRID=20171015204552-c792b887-b1a6-11e7-9788-525400f775ce', 'Host': 'www.lagou.com', 'Origin': 'https://www.lagou.com', 'Referer': 'https://www.lagou.com/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36', &#125; &#125; ps: 迷之成功, 测试了好多遍用类似的参数都不行, 只能用这个办法了.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XPath简单语法]]></title>
      <url>%2F2018%2F01%2F11%2FXPath%E7%AE%80%E5%8D%95%E8%AF%AD%E6%B3%95%2F</url>
      <content type="text"><![CDATA[简单记记 获取div//div 获取指定属性 div//div[@class=&quot;top&quot;] 获取div属性中的文本//div/text() 获取div的指定属性//div/@class 获取div标签下所有文本string(//div)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Scrapy配置文件Setting理解]]></title>
      <url>%2F2018%2F01%2F04%2FScrapy%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6Setting%E7%90%86%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[Setting.py 选项理解 并发数控制 并发是指同时处理的request的数量。其有全局限制和局部(每个网站)的限制。Scrapy默认的全局并发限制对同时爬取大量网站的情况并不适用，因此您需要增加这个值。 增加多少取决于您的爬虫能占用多少CPU。 一般开始可以设置为 100 。不过最好的方式是做一些测试，获得Scrapy进程占取CPU与并发数的关系。 为了优化性能，您应该选择一个能使CPU占用率在80%-90%的并发数CONCURRENT_REQUESTS = 100，scrapy中默认的并发数是32 默认每个域名的并发数：8 CONCURRENT_REQUESTS_PER_DOMAIN = 8 每个IP的最大并发数：0表示忽略 CONCURRENT_REQUESTS_PER_IP = 0 减少下载超时 如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。DOWNLOAD_TIMEOUT = 15,其中15是设置的下载超时时间 配置ITEM处理 ITEM_PIPELINES = { &#39;lagou.pipemysql.MySQLAsyncPipeline&#39;: 401 #数字代表优先级 } 配置MIDDLEWARES DOWNLOADER_MIDDLEWARES = { &#39;lagou.middlewares.ProxyMiddleware&#39;: 543, }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2018]]></title>
      <url>%2F2018%2F01%2F02%2F2018%2F</url>
      <content type="text"><![CDATA[小小规划下2018(假装假装)First稳定工作 爬虫向 互联网金融 10K+ Second攒钱 3000 * 12 + 7000 = 4W+ Third技能树(不分先后): Scrapy精通 数据挖掘+机器学习入门 数据结构重刷 MySQL精通 分布式精通 (CSDN视频) Linux熟练使用 (鸟哥私房菜) Python基础 愿望清单: 微单 4K 显示器 2K PS4 2K 内存条 0.5K 尾巴: 算了算了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Scrapy框架流程图]]></title>
      <url>%2F2017%2F12%2F16%2FScrapy%E6%A1%86%E6%9E%B6%E6%B5%81%E7%A8%8B%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[方便理解整个Scrapy运行流程 画个图]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[时间戳转换]]></title>
      <url>%2F2017%2F11%2F22%2F%E6%97%B6%E9%97%B4%E6%88%B3%E8%BD%AC%E6%8D%A2%2F</url>
      <content type="text"><![CDATA[转自:http://blog.csdn.net/xiaobing_blog/article/details/12591917]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[requests下载pdf]]></title>
      <url>%2F2017%2F11%2F15%2F%E4%B8%8B%E8%BD%BDpdf%2F</url>
      <content type="text"><![CDATA[requests下载pdf123456import requestsresponse = requests.get(url,stream=&quot;TRUE&quot;)with open(&apos;C:\\Users\\jojo\\Desktop&apos;+file_name+&apos;.pdf&apos;, &apos;wb&apos;) as file: for data in response.iter_content(): file.write(data) file.close()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL导出EXCLE文件]]></title>
      <url>%2F2017%2F10%2F07%2FMySQL%E5%AF%BC%E5%87%BAEXCLE%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[MySQL导出EXCLE文件一般用命令导出会遇到问题:The MySQL server is running with the --secure-file-priv option so it cannot execute this statement报错原因：secure_file_priv设置了指定目录，需要在指定的目录下进行数据导出 Windows下my.ini所在路径:C:\ProgramData\MySQL\MySQL Server 5.7 secure_file_priv这个变量不支持动态修改，官方文档写明了，需要重启生效 命令:导出文件SELECT * FROM MY_TABLE INTO OUTFILE &#39;FILE_PATH&#39;;导入文件LOAD DATA INFILE &#39;FILE_PATH&#39; INTO TABLE MY_TABLE; 参考资料: MySQL配置修改mysql导出导入文件问题整理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[selenium设置chrome和phantomjs的请求头信息]]></title>
      <url>%2F2017%2F09%2F16%2Fselenium%E8%AE%BE%E7%BD%AEchrome%E5%92%8Cphantomjs%E7%9A%84%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%BF%A1%E6%81%AF%2F</url>
      <content type="text"><![CDATA[目录一：selenium设置phantomjs请求头：二：selenium设置chrome请求头：三：selenium设置chrome–cookie：四：selenium设置phantomjs-图片不加载： selenium设置phantomjs请求头：1234567891011from selenium import webdriverfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilitiesdcap = dict(DesiredCapabilities.PHANTOMJS)dcap[&quot;phantomjs.page.settings.userAgent&quot;] = (&quot;Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.23 Mobile Safari/537.36&quot;)driver = webdriver.PhantomJS(desired_capabilities=dcap)driver.get(&quot;https://httpbin.org/get?show_env=1&quot;)driver.get_screenshot_as_file(&apos;01.png&apos;)driver.quit() selenium设置chrome请求头：1234567891011from selenium import webdriver# 进入浏览器设置options = webdriver.ChromeOptions()# 设置中文options.add_argument(&apos;lang=zh_CN.UTF-8&apos;)# 更换头部options.add_argument(&apos;user-agent=&quot;Mozilla/5.0 (iPod; U; CPU iPhone OS 2_1 like Mac OS X; ja-jp) AppleWebKit/525.18.1 (KHTML, like Gecko) Version/3.1.1 Mobile/5F137 Safari/525.20&quot;&apos;)browser = webdriver.Chrome(chrome_options=options)url = &quot;https://httpbin.org/get?show_env=1&quot;browser.get(url)browser.quit() selenium设置chrome–cookie：cookie用于模拟登陆 123456789101112131415from selenium import webdriverbrowser = webdriver.Chrome()url = &quot;https://www.baidu.com/&quot;browser.get(url)# 通过js新打开一个窗口newwindow=&apos;window.open(&quot;https://www.baidu.com&quot;);&apos;# 删除原来的cookiebrowser.delete_all_cookies()# 携带cookie打开browser.add_cookie(&#123;&apos;name&apos;:&apos;ABC&apos;,&apos;value&apos;:&apos;DEF&apos;&#125;)# 通过js新打开一个窗口browser.execute_script(newwindow)input(&quot;查看效果&quot;)browser.quit() selenium设置phantomjs-图片不加载：12345678910111213141516from selenium import webdriveroptions = webdriver.ChromeOptions()prefs = &#123; &apos;profile.default_content_setting_values&apos;: &#123; &apos;images&apos;: 2 &#125;&#125;options.add_experimental_option(&apos;prefs&apos;, prefs)browser = webdriver.Chrome(chrome_options=options)# browser = webdriver.Chrome()url = &quot;http://image.baidu.com/&quot;browser.get(url)input(&quot;是否有图&quot;)browser.quit() 转载自URl-team]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Beautiful Soup]]></title>
      <url>%2F2017%2F09%2F14%2FBeautiful-Soup%2F</url>
      <content type="text"><![CDATA[Beautiful Soup4是Python的一个第三方库，用来从HTML和XML中提取数据。Beautiful Soup4在某些方面比Xpath易懂，但是不如Xpath简洁，而且由于它是使用Python开发的，因此速度比Xpath慢。 使用规范解析源代码,用lxml解析器soup = BeautifulSoup(source, &#39;lxml&#39;) 查找内容查找内容的方法和使用XPath非常相似。首先找的包含特殊属性值的标签，并使用这个标签来寻找内容。 查找到有用的内容，然后在这个内容的基础上，继续查找下面的内容。find_all 与find的不同在于，find_all返回的是列表，如果没有找到，就会返回空列表。而find返回的直接是一个BeautifulSoup Tag对象，如果有多个符合条件的BeautifulSoup Tag对象，则返回第一个对象，如果找不到，就会返回None。 find_all( name , attrs , recursive , text , **kwargs ) name就是HTML的标签名，类似于body, div, ul, li之类。 attrs参数的值是一个字典，字典的key是属性名，字典的value是属性值：find_all(attrs={&#39;class&#39;: &#39;useful&#39;}) recursive的值为True或者False，当它为False的时候，Beautiful Soup不会搜索子节点。 text可以是一个字符串或者是正则表达式。用于搜索标签里面的文本信息：find_all(text=re.compile(&#39;我需要&#39;)) **kwargs表示key=value形式的参数。一般这里的key是属性，value是属性值。这个大多数情况下与标签配合使用，但是有时候如果属性值非常特殊，也可以单独使用：find_all(&#39;div&#39;, id=&#39;test&#39;) find_all(class_=&#39;iamstrange&#39;) 使用范例用笔趣看网站搜索页面做范例 一念永恒搜索12345678910111213141516171819202122232425from bs4 import BeautifulSoupimport requestsurl=&apos;http://zhannei.baidu.com/cse/search?ie=gbk&amp;s=2758772450457967865&amp;q=%D2%BB%C4%EE%D3%C0%BA%E3&apos;source=requests.get(url).content.decode()# 解析源代码soup = BeautifulSoup(source, &apos;lxml&apos;)# 解析出小说信息,得到的是一个列表divs=soup.find_all(&apos;div&apos;,class_=&apos;result-item result-game-item&apos;)# 遍历列表项,继续获取其他详细信息for div in divs: div_soup=BeautifulSoup(str(div),&apos;lxml&apos;) # 获取 class 为特定值的 a 标签 [属性内容] name=div_soup.find(name=&apos;a&apos;,class_=&apos;result-game-item-title-link&apos;)[&apos;title&apos;] print(&apos;小说名称:&apos;+name) # 结尾加 .text表示获取其中文字 info=div_soup.find(name=&apos;p&apos;,class_=&apos;result-game-item-desc&apos;).text print(&apos;简介:&apos;+info) auth=div_soup.find(name=&apos;p&apos;,class_=&apos;result-game-item-info-tag&apos;).text # 用replace方法可以出去空格或换行符 auth=auth.replace(&apos; &apos;,&apos;&apos;).replace(&apos;\r&apos;,&apos; &apos;) print(auth)print(&apos;----------------------------&apos;)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[登录后下载图片]]></title>
      <url>%2F2017%2F09%2F02%2F%E7%99%BB%E5%BD%95%E5%90%8E%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%2F</url>
      <content type="text"><![CDATA[竟然有丧心病狂的网站连图片都要登录之后才能下载…记录下: post模拟登录 response获取getEntity() entity.getEntity() 转换成inputstream流 写入图片代码: 123456789101112131415161718192021222324252627282930313233343536373839public class Test_downhttp &#123; public static void main(String[] args) &#123; CloseableHttpClient httpclient = HttpClients.createDefault(); HttpPost httpPost = new HttpPost(&quot;http://dzb.csxww.com/json/login.action&quot;); List&lt;NameValuePair&gt; valuePairs = new LinkedList&lt;NameValuePair&gt;(); valuePairs.add(new BasicNameValuePair(&quot;username&quot;, &quot;******&quot;)); valuePairs.add(new BasicNameValuePair(&quot;password&quot;, &quot;******&quot;)); // 向对方服务器发送Post请求 // 将参数进行封装，提交到服务器端 UrlEncodedFormEntity entity = new UrlEncodedFormEntity(valuePairs, Consts.UTF_8); httpPost.setEntity(entity); HttpGet httpget = new HttpGet(&quot;http://dzb.csxww.com/articlepic.action?dgId=1178765179&amp;pageId=37&amp;type=article&quot;); try &#123; httpclient.execute(httpPost); CloseableHttpResponse response = httpclient.execute(httpget); HttpEntity entitty = response.getEntity(); //写入图片 InputStream in = entitty.getContent(); FileOutputStream fileOutputStream = null; byte[] data = new byte[1024]; int len = 0; fileOutputStream = new FileOutputStream(&quot;E:\\test1.png&quot;); while ((len = in.read(data)) != -1) &#123; fileOutputStream.write(data, 0, len); &#125; &#125; catch (ClientProtocolException e) &#123; // TODO 自动生成的 catch 块 e.printStackTrace(); &#125; catch (IOException e) &#123; // TODO 自动生成的 catch 块 e.printStackTrace(); &#125; // 登录 &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[笔趣看小说下载(python)]]></title>
      <url>%2F2017%2F08%2F30%2F%E7%AC%94%E8%B6%A3%E7%9C%8B%E5%B0%8F%E8%AF%B4%E4%B8%8B%E8%BD%BD-python%2F</url>
      <content type="text"><![CDATA[笔趣看网站只支持阅读不支持下载,通过python抓取页面链接,做到下载小说的功能 拼接链接search_url = r&#39;http://zhannei.baidu.com/cse/search?q=&#39; + book_name + &#39;&amp;click=1&amp;s=2758772450457967865&amp;nsid=&#39;其中book_name为在控制台输入的书名因为url中不能出现中文,所以要先编码一次.编码方法: 先导入quotefrom urllib.parse import quote search_url = quote(search_url, safe = string.printable) string.printable safe表示可以忽略的字符 抓取搜索页第一项搜索结果 设置头 12head = &#123;&#125;head[&apos;User-Agent&apos;] = &apos;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19&apos; 读取网页数据 123req = request.Request(search_url, headers=head)res = request.urlopen(req)html = res.read().decode(&apos;utf-8&apos;) 用BeautifulSoup解析 123book_name_soup = BeautifulSoup(html, &apos;lxml&apos;)div = book_name_soup.find_all(&apos;div&apos;,class_=&apos;game-legend-a&apos;)a_soup = BeautifulSoup(str(div), &apos;lxml&apos;) 用正则匹配出小说链接 1234regex = r&quot;location=&apos;([\s\S]*?)&apos;\&quot;&quot;matches=re.search(regex,str(a_soup))# 书本链接为第一个搜索结果a_href=matches.group(1) 读取链接内容,解析出所有章节url并把内容写入TXT 123456789101112131415161718192021222324252627282930313233343536373839404142def downTXT(book_name): bookList_url = selectBook(book_name) file = open(book_name + &apos;.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) head = &#123;&#125; head[ &apos;User-Agent&apos;] = &apos;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19&apos; target_req=request.Request(bookList_url,headers=head) target_res=request.urlopen(target_req) target_html=target_res.read().decode(&apos;gbk&apos;) listmain_soup=BeautifulSoup(target_html,&apos;lxml&apos;) chapters=listmain_soup.find_all(&apos;div&apos;,class_=&apos;listmain&apos;) zhengwen_soup=BeautifulSoup(str(chapters),&apos;lxml&apos;) zhengwen_flag=False index=1 numbers=(len(zhengwen_soup.dl.contents)-1)/2 for child in zhengwen_soup.dl.children: if child!=&apos;\n&apos;: if child.string==r&apos;《&apos;+book_name+&apos;》正文卷&apos;: zhengwen_flag=True if zhengwen_flag==True and child.a!=None: download_url=&apos;http://www.biqukan.com/&apos;+child.a.get(&apos;href&apos;) download_req=request.Request(download_url,headers=head) download_res=request.urlopen(download_req) download_html=download_res.read().decode(&apos;gbk&apos;,&apos;ignore&apos;) download_name=child.string texts_soup=BeautifulSoup(download_html,&apos;lxml&apos;) texts=texts_soup.find_all(id=&apos;content&apos;,class_=&apos;showtxt&apos;) text_soup=BeautifulSoup(str(texts),&apos;lxml&apos;) write_flag=True file.write(download_name+&apos;\n\n&apos;) for each in text_soup.div.text.replace(&apos;\xa0&apos;,&apos;&apos;): if each==&apos;h&apos;: write_flag=False if write_flag==True and each!=&apos; &apos;: file.write(each) if write_flag==True and each==&apos;\r&apos;: file.write(&apos;\n&apos;) file.write(&apos;\n\n&apos;) print(&quot;已下载:&quot; + str((index / numbers) * 100) + &apos;\r&apos;) index += 1 file.close() Github源码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[抓取天猫价格]]></title>
      <url>%2F2017%2F08%2F16%2F%E6%8A%93%E5%8F%96%E5%A4%A9%E7%8C%AB%E4%BB%B7%E6%A0%BC%2F</url>
      <content type="text"><![CDATA[天猫商品详情页是用Ajax动态加载的 先后台抓取url请求链接 添加referer得到实体 最后解析JSon即可 代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.io.IOException;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.util.EntityUtils;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;public class TmailGoodsInfo &#123; public static void main(String[] args) throws ClientProtocolException, IOException &#123; String url = &quot;https://mdskip.taobao.com/core/initItemDetail.htm?isForbidBuyItem=false&amp;cartEnable=true&amp;itemId=549004787468&amp;isPurchaseMallPage=false&amp;offlineShop=false&amp;queryMemberRight=true&amp;isSecKill=false&amp;sellerPreview=false&amp;cachedTimestamp=1502846997903&amp;tmallBuySupport=true&amp;isApparel=false&amp;addressLevel=2&amp;service3C=false&amp;showShopProm=false&amp;isUseInventoryCenter=false&amp;isRegionLevel=false&amp;isAreaSell=false&amp;tryBeforeBuy=false&amp;household=false&amp;callback=setMdskip&amp;timestamp=1502848785256&amp;isg=null&amp;isg2=AgkJZHnWtCmLwUg0MZ3S7hxgDTXPRK_NbEaJ26t86vBw8isE86JLWVliQGEq&quot;; String referer = &quot;https://detail.tmall.com/item.htm?id=549004787468&amp;ali_refid=a3_430583_1006:1121371980:N:java:6628cce7b7279c82556f8802f07106ec&amp;ali_trackid=1_6628cce7b7279c82556f8802f07106ec&amp;spm=a230r.1.14.1.76bf523hqYFaJ&quot;; CloseableHttpClient httpclient = HttpClients.createDefault(); HttpGet httpGet = new HttpGet(url); httpGet.setHeader(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36&quot; + &quot; (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&quot;); // 添加referer httpGet.setHeader(&quot;Referer&quot;, referer); CloseableHttpResponse response = httpclient.execute(httpGet); String entity = EntityUtils.toString(response.getEntity(), &quot;UTF-8&quot;); entity = entity.substring(12, entity.length() - 1); // 解析JSON JSONObject object = JSON.parseObject(entity); JSONObject object2 = (JSONObject) object.get(&quot;defaultModel&quot;); JSONObject object3 = (JSONObject) object2.get(&quot;itemPriceResultDO&quot;); JSONObject object4 = (JSONObject) object3.get(&quot;priceInfo&quot;); JSONObject object5 = (JSONObject) object4.get(&quot;def&quot;); JSONArray jsonArray = JSON.parseArray(object5.get(&quot;promotionList&quot;).toString()); // System.out.println(object5); if (jsonArray.size() == 1) &#123; JSONObject object6 = (JSONObject) jsonArray.get(0); System.out.println(&quot;实际售价为:&quot; + object6.get(&quot;price&quot;)); &#125; &#125;&#125; 参考自: java爬虫抓取天猫商品的价格数据]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[读取XML内容]]></title>
      <url>%2F2017%2F08%2F15%2F%E8%AF%BB%E5%8F%96XML%E5%86%85%E5%AE%B9%2F</url>
      <content type="text"><![CDATA[很多时候需要为程序设置全局变量或者经常需要修改的量,这时候放到XML配置文件里就很有必要了 需要用的的包 dom4j 下载地址代码如下 123456789101112131415161718192021222324252627282930313233343536373839import java.io.File;import java.util.Iterator;import java.util.List;import org.dom4j.Document;import org.dom4j.Element;import org.dom4j.io.SAXReader;public class ReadXMLDemo &#123; public static void main(String[] args) throws Exception &#123; // 读取XML文件,获得document对象 SAXReader reader = new SAXReader(); Document document = reader.read(new File(&quot;src/test.xml&quot;)); // 取得节点对象 Element root = document.getRootElement();// 取得根节点 Element books = root.element(&quot;Books&quot;); // 取得该节点下 &quot;Books&quot; 子节点 Element book = books.element(&quot;Book&quot;); System.out.println(book.getName() + book.getText()); // getName()获取该节点名字,getText()取得该节点内容 System.out.println(book.attribute(&quot;id&quot;).getText()); //book.attribute(&quot;id&quot;)获取该节点下属性为id的内容 // 遍历子节点下所有元素 List&lt;Element&gt; bookList = book.elements(); for (Element element : bookList) &#123; System.out.println(element.getName() + &quot; :&quot; + element.getText()); &#125; //遍历子节点下子节点所有元素 for (Iterator it = books.elementIterator(); it.hasNext();) &#123; Element element = (Element) it.next(); List&lt;Element&gt; booksList = element.elements(); for (Element element2 : booksList) &#123; System.out.println(element2.getName() + &quot;: &quot; + element2.getText()); &#125; &#125; &#125;&#125; XML文件如下:123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Info&gt; &lt;Books&gt; &lt;Book id=&quot;001&quot;&gt; &lt;Name&gt;Java&lt;/Name&gt; &lt;Price&gt;50.0&lt;/Price&gt; &lt;/Book&gt; &lt;Book id=&quot;002&quot;&gt; &lt;Name&gt;Python&lt;/Name&gt; &lt;Price&gt;30.0&lt;/Price&gt; &lt;/Book&gt; &lt;/Books&gt; &lt;Phones&gt; &lt;Phone id=&quot;001&quot;&gt; &lt;Name&gt;三星S8&lt;/Name&gt; &lt;Price&gt;5588.0&lt;/Price&gt; &lt;/Phone&gt; &lt;/Phones&gt; &lt;/Info&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[eclipse常用快捷键]]></title>
      <url>%2F2017%2F08%2F14%2Feclipse%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
      <content type="text"><![CDATA[总结下常用快捷键吧,偶尔会忘ALT + Shift + J 文档注释Ctr + Shift + R 打开资源 (很好用)Alt + Shift + R 重命名 (很好用)Ctr + Shift + O 快速导入所有包 (很好用)Ctr + Shift + F 格式化代码Alt + / 代码提示Ctr + 1 错误修正建议Ctr + D 删除当前行Ctr + / 注释当前行,再按则取消注释]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[模拟登录(Post)]]></title>
      <url>%2F2017%2F08%2F11%2F%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95(Post)%2F</url>
      <content type="text"><![CDATA[用Cookie登录终究不是办法,总会过期的嘛.终于遇到了用Post解决的时候,忙碌的一天… 网站:乐清日报以抓乐清日报为例.工具:Fideler首先找到登录页面:http://www.yqrb.cn/check/UserLogin.aspx打开Fiddler准备抓数据. 输入用户名密码点击确认 看到Fiddler页面其中WebForms是发送出去的数据 开始写代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.apabi.crawler;import org.apache.http.Consts;import org.apache.http.Header;import org.apache.http.NameValuePair;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.config.CookieSpecs;import org.apache.http.client.config.RequestConfig;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.client.methods.HttpPost;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import org.apache.http.util.EntityUtils;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;public class PostDemo &#123; public static void main(String[] args) throws ClientProtocolException, IOException&#123; //创建默认客户端 CloseableHttpClient closeableHttpClient = HttpClients.createDefault(); //创建Post请求实例 HttpPost httpPost = new HttpPost(&quot;http://www.yqrb.cn/check/UserLogin.aspx&quot;); //创建参数列表 List&lt;NameValuePair&gt; valuePairs = new LinkedList&lt;NameValuePair&gt;(); valuePairs.add(new BasicNameValuePair(&quot;__VIEWSTATE&quot;, &quot;/wEPDwUKLTYyMjc0MTMwNWQYAQUeX19Db250cm9sc1JlcXVpcmVQb3N0QmFja0tleV9fFgIFEExvZ2luMSRDaGVja0JveDEFD0xvZ2luMSRpYnRMb2dpblgHoKZ58APGc4h5Aq8tcf+WuKlv&quot;)); valuePairs.add(new BasicNameValuePair(&quot;Login1$txtUserName&quot;, &quot;aaaa&quot;)); valuePairs.add(new BasicNameValuePair(&quot;Login1$txtUserPassWord&quot;, &quot;bbbb&quot;)); valuePairs.add(new BasicNameValuePair(&quot;Login1$CheckBox1&quot;, &quot;on&quot;)); valuePairs.add(new BasicNameValuePair(&quot;Login1$ibtLogin.x&quot;, &quot;0&quot;)); valuePairs.add(new BasicNameValuePair(&quot;Login1$ibtLogin.y&quot;, &quot;0&quot;)); valuePairs.add(new BasicNameValuePair(&quot;__EVENTVALIDATION&quot;, &quot;/wEWBQL2/JnoBQLGrKnLCQLZ3e7ECAKUkrPDCgKGkYz4DFU4WWL55rJshJkm7OKlQUqgsNqQ&quot;)); //向对方服务器发送Post请求 //将参数进行封装，提交到服务器端 UrlEncodedFormEntity entity = new UrlEncodedFormEntity(valuePairs, Consts.UTF_8); httpPost.setEntity(entity); closeableHttpClient.execute(httpPost);//登录 //登录成功之后就可以开始抓页面数据了 HttpGet g = new HttpGet(&quot;http://www.yqrb.cn/html/2017-08/10/content_342754.htm&quot;);//获取“我关注的问题”页面 CloseableHttpResponse r = closeableHttpClient.execute(g); System.out.println(EntityUtils.toString(r.getEntity()));// System.out.println(EntityUtils.toString(httpResponse.getEntity())); //如果模拟登录成功// if(httpResponse.getStatusLine().getStatusCode() == 200) &#123;// HttpGet httpGet = new HttpGet(&quot;http://www.yqrb.cn/html/2017-08/10/content_342754.htm&quot;);// httpResponse = closeableHttpClient.execute(httpGet);//// Header[] headers = httpResponse1.getAllHeaders();//// for (Header header : headers) &#123;//// System.out.println(header.getName() + &quot;: &quot; + header.getValue());//// &#125;// System.out.println(EntityUtils.toString(httpResponse.getEntity())); &#125; &#125; 重点://创建参数列表 List&lt;NameValuePair&gt; valuePairs = new LinkedList&lt;NameValuePair&gt;();Post数据要写全 ``//将参数进行封装，提交到服务器端 UrlEncodedFormEntity entity = new UrlEncodedFormEntity(valuePairs, Consts.UTF_8); httpPost.setEntity(entity); closeableHttpClient.execute(httpPost);//执行登录 `` 注释写的还是比较清楚的,纪录收藏下吧. ps:脑仁疼 不过终于搞定啦 哈哈哈哈哈 ╭(●｀∀´●)╯╰(●’◡’●)╮ (●’◡’●)ﾉ ヾ(´▽‘)ﾉ]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[模拟登录(Cookie)]]></title>
      <url>%2F2017%2F08%2F09%2F%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%2F</url>
      <content type="text"><![CDATA[很多网站需要登录才能看到具体内容,所以需要模拟表头登录后抓取模拟Cookie登录抓取内容1234567891011121314151617181920212223242526272829303132333435363738import java.io.IOException;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.util.EntityUtils;public class Aifou &#123; public static void main(String[] args) &#123; CloseableHttpClient closeableHttpClient = HttpClients.createDefault(); HttpGet httpGet = new HttpGet(&quot;http://www.aifou.cn&quot;); httpGet.setHeader(&quot;Accept&quot;, &quot;text/html,application/xhtml+xml,&quot; + &quot;application/xml;q=0.9,image/webp,*/*;q=0.8&quot;); httpGet.setHeader(&quot;Accept-Encoding&quot;, &quot;gzip, deflate, sdch, br&quot;); httpGet.setHeader(&quot;Accept-Language&quot;, &quot;zh-CN,zh;q=0.8&quot;); // 重点在Cookie 通过登录成功后的页面控制台复制cookie即可 httpGet.setHeader(&quot;Cookie&quot;, &quot;&quot;); httpGet.setHeader(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36&quot; + &quot; (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&quot;); try &#123; // 客户端执行httpGet方法，返回响应 CloseableHttpResponse closeableHttpResponse = closeableHttpClient.execute(httpGet); // 得到服务响应状态码 if (closeableHttpResponse.getStatusLine().getStatusCode() == 200) &#123; // 得到响应实体 String entity = EntityUtils.toString(closeableHttpResponse.getEntity(), &quot;utf-8&quot;); System.out.println(entity); &#125; else &#123; &#125; &#125; catch (ClientProtocolException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 重点为Cookie的信息,需要注意的是Cookie会过期,过期的话就会失败]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HttpClient基本使用]]></title>
      <url>%2F2017%2F08%2F07%2FHttpClient%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[做了一个月爬虫了,从头整理下吧,方便以后查询使用.123456789101112131415161718192021222324252627import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;public class HttpClientDemo &#123; public static void main(String[] args) throws Exception &#123; // 创建默认的客户端实例 CloseableHttpClient httpClient = HttpClients.createDefault(); // 创建get请求实例 HttpGet httpget = new HttpGet(&quot;http://www.baidu.com&quot;); System.out.println(&quot;executing request &quot; + httpget.getURI()); try &#123; // 客户端执行get请求 返回响应 CloseableHttpResponse response = httpClient.execute(httpget); // 服务器响应状态行 System.out.println(response.getStatusLine().toString()); &#125; finally &#123; httpClient.close(); &#125; &#125;&#125; 简单理解下:CloseableHttpClient httpClient = HttpClients.createDefault(); 上述代码我们可以理解为我先打开了一个“浏览器”，注意，并不是真正意义上的浏览器，只是进行了这步操作，表明我们马上就可以访问网页了。 HttpGet httpget = new HttpGet(&quot;http://www.baidu.com&quot;); 上述代码就表示了我们请求了一个页面，用的是get方法，对于请求页面使用的是哪一种方法，一般来说，只有在进行登录页面的时，我们会使用Post方法。我们可以通过chrome开发者工具来了解对于这个页面是使用了哪个方法。 我们理解为上面两个步骤就是打开一个浏览器，然后在地址栏输入了一个网址。 CloseableHttpResponse response = httpClient.execute(httpget);上述代码相当于我们此时在浏览器中按下了回车，URL资源所在的服务器就会开始给你返回这个网页的数据。包括请求头，消息实体等等.最后在finally释放资源. HttpClient jar下载参考自:Hg_Yi]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[消愁]]></title>
      <url>%2F2017%2F08%2F03%2F%E6%B6%88%E6%84%81%2F</url>
      <content type="text"><![CDATA[当你走进这欢乐场背上所有的梦与想各色的脸上各色的妆没人记得你的模样三巡酒过你在角落固执的唱着苦涩的歌听他在喧嚣里被淹没你拿起酒杯对自己说一杯敬朝阳，一杯敬月光唤醒我的向往，温柔了寒窗于是可以不回头的逆风飞翔不怕心头有雨，眼底有霜一杯敬故乡，一杯敬远方守着我的善良，催着我成长所以南北的路从此不再漫长灵魂不再无处安放一杯敬明天，一杯敬过往支撑我的身体，厚重了肩膀虽然从不相信所谓山高水长人生苦短何必念念不忘一杯敬自由，一杯敬死亡宽恕我的平凡，驱散了迷惘好吧天亮之后总是潦草离场清醒的人最荒唐好吧天亮之后总是潦草离场清醒的人最荒唐—— 毛不易消愁-QQ音乐]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[碎碎念]]></title>
      <url>%2F2017%2F07%2F29%2F%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
      <content type="text"><![CDATA[广厦万间,夜眠八尺.珍馐百味,不过一饱.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Git从零开始]]></title>
      <url>%2F2017%2F07%2F26%2FGit%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%2F</url>
      <content type="text"><![CDATA[从头整理下Git流程 准备 安装Git后,创建仓库目录文件夹,打开命令行输入ssh-keygen -t rsa -C &quot;邮箱名&quot; 创建SSH密钥。 在GitHub账号设置中添加SSH许可。 ssh -T git@github.com连接GitHub 修改个人信息 git config --global user.name &quot;名字&quot; git config --global user.email &quot;邮箱&quot; 开始 git init 初始化本地仓库 git clone url 下载远程Github仓库,url为github clone链接 基本操作: git add file_name 将文件添加到缓存. *为全部文件 git status 查看项目当前状态 git status -s获得简短输出 git diff查看已写入缓存与已修改但尚未写入缓存的改动的区别. 例如修改了文件里的内容后,git status 显示你上次提交更新后的更改或者写入缓存的改动， 而 git diff 一行一行地显示这些改动具体是啥。 git commit将缓存区内容添加到仓库中.git commit -m &quot;注释&quot;可以简单说明改动. git rm file将条目从缓存区删除,包括文件本身.如果要在工作目录中保留文件用 git rm --cached git mv 命令用于移动或重命名一个文件、目录、软连接。 分支管理: git branch (branchname)创建一个新的分支 git checkout 切换分支,在切换分支时会用该分支最后提交的快照替换工作目录的内容,所以多个分支不需要多个目录. git branch列出本地分支,*为当前所在分支 git branch -d (branchname)删除分支 git merge branchname合并分支内容 查看提交历史 git log查看历史信息,可以用git log --oneline查看简明信息 简单流程 创建本地仓库目录文件夹 创建GitHub远程仓库,git clone url克隆到本地 git init # 初始化 git add README.md # 添加文件 git commit -m &quot;添加注释信息&quot; # 提交并备注信息 git push提交到远程仓库 ps.只是最最最基础的流程,分支慢慢研究了. ##最后会随着看的越来越多慢慢更新的,先熟悉下.贴两个链接: 简单命令加流程GitHub简明教程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式-单例模式（饿汉模式or懒汉模式）]]></title>
      <url>%2F2017%2F06%2F27%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%EF%BC%88%E9%A5%BF%E6%B1%89%E6%A8%A1%E5%BC%8For%E6%87%92%E6%B1%89%E6%A8%A1%E5%BC%8F%EF%BC%89%2F</url>
      <content type="text"><![CDATA[在从头研究项目的时候发现存储配置文件字段用到了饿汗单例模式，深入研究看看。 ##单例模式单例模式就是在应用程序中只创建一个该类的对象。这个设计模式主要目的是想在整个系统中只能出现一个类的实例。比如你的软件的全局配置信息，或者是一个Factory，或是一个主控类，等等。你希望这个类在整个系统中只能出现一个实例。当然，作为一个技术负责人的你，你当然有权利通过使用非技术的手段来达到你的目的。比如：你在团队内部明文规定，“XX类只能有一个全局实例，如果某人使用两次以上，那么该人将被处于2000元的罚款！”（呵呵），你当然有权这么做。但是如果你的设计的是东西是一个类库，或是一个需要提供给用户使用的API，恐怕你的这项规定将会失效。因为，你无权要求别人会那么做。所以，这就是为什么，我们希望通过使用技术的手段来达成这样一个目的的原因。 ###饿汉模式和懒汉模式 饿汉 类加载时候就已经完成了初始化，不需要同步懒汉 类加载的时候不初始化，需要同步 懒汉模式：类加载时不初始化，因此在类加载时速度快，但运行时获取对象的速度慢。例如：12345678910public class Singleton &#123; private static Singleton singleton = null; private Singleton() &#123; &#125; //私有的构造函数，表明这个类不可能形成实例。 public static Singleton getInstance() &#123; //借助getInstance()让其形成实例。 if (singleton== null) &#123; singleton= new Singleton(); &#125; return singleton; &#125;&#125; 注意： 即然这个类是不可能形成实例，那么，我们需要一个静态的方式让其形成实例：getInstance()。注意这个方法是在new自己，因为其可以访问私有的构造函数，所以他是可以保证实例被创建出来的。 在getInstance()中，先做判断是否已形成实例，如果已形成则直接返回，否则创建实例。 取实例时，只需要使用Singleton.getInstance()就行了。 饿汉模式：在类加载时就完成了初始化，所以类加载较慢，但获取对象的速度快例如：12345678910111213public class EagerSingleton &#123; private static EagerSingleton instance = new EagerSingleton();//静态私有成员，已初始化 private EagerSingleton() &#123; //私有构造函数 &#125; //静态，不用同步（类加载时已初始化，不会有多线程的问题） public static EagerSingleton getInstance() &#123; return instance; &#125;&#125; ##单例模式的多线程多线程：双重检查123456789101112131415public class Singleton&#123; private volatile static Singleton singleton = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (singleton== null) &#123; synchronized (Singleton.class) &#123; if (singleton== null) &#123; singleton= new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 第一个条件判断实例是否已经创建,如果创建了则不需要同步,直接返回就好,否则开始同步线程. 第二个条件判断,如果被同步的线程中,有一个线程创建了对象,那么别的线程就不用再创建了. 注意:此处volatile的作用 这个变量不会在多个线程中存在复本，直接从内存读取。 这个关键字会禁止指令重排序优化。也就是说，在 volatile 变量的赋值操作后面会有一个内存屏障（生成的汇编代码上），读操作不会被重排序到内存屏障之前。 还有一种《Effective Java》推荐方式静态内部类的方法123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 小结：静态内部类还是不太懂，后续慢慢思考一下。 参考资料：深入浅出单实例SINGLETON设计模式 http://coolshell.cn/articles/265.html如何正确地写出单例模式 http://wuchong.me/blog/2014/08/28/how-to-correctly-write-singleton-pattern/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[线程与进程简单理解]]></title>
      <url>%2F2017%2F05%2F09%2F%E7%BA%BF%E7%A8%8B%E4%B8%8E%E8%BF%9B%E7%A8%8B%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[由于线程和进程比较抽象，不好理解，看到篇文章，简单记录下，方便理解。 计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。 假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。 进程就像工厂的车间，它代表CPU所能处理的单个任务。车间可以有很多个，任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。 一个车间里，可以有很多工人。他们协同完成一个任务。线程就好比车间里的工人。一个进程可以包括多个线程。 车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。 可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。 一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫“互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。 还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。 这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做“信号量”（Semaphore），用来保证多个线程不会互相冲突。不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。 总结：操作系统的设计，因此可以归结为三点：（1）以多进程形式，允许多个任务同时运行；（2）以多线程形式，允许单个任务分成不同的部分运行；（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java 中的序列化]]></title>
      <url>%2F2017%2F04%2F19%2Fjava%20%E4%B8%AD%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
      <content type="text"><![CDATA[序列化是干什么的？ 简单说就是为了保存在内存中的各种对象的状态，并且可以把保存的对象状态再读出来。虽然你可以用你自己的各种各样的方法来保存Object States，但是Java给你提供一种应该比你自己好的保存对象状态的机制,那就是序列化。 什么情况下需要序列化1.当你想把的内存中的对象保存到一个文件中或者数据库中时候；2.当你想用套接字在网络上传送对象的时候；3.当你想通过RMI传输对象的时候； 当对一个对象实现序列化时，究竟发生了什么？在没有序列化前，每个保存在堆（Heap）中的对象都有相应的状态（state），即实例变量（instance ariable）比如：123Foo myFoo = new Foo(); myFoo .setWidth(37); myFoo.setHeight(70); 当通过下面的代码序列化之后，MyFoo对象中的width和Height实例变量的值（37，70）都被保存到foo.ser文件中，这样以后又可以把它 从文件中读出来，重新在堆中创建原来的对象。当然保存时候不仅仅是保存对象的实例变量的值，JVM还要保存一些小量信息，比如类的类型等以便恢复原来的对象。 123FileOutputStream fs = new FileOutputStream(&quot;foo.ser&quot;);ObjectOutputStream os = new ObjectOutputStream(fs); os.writeObject(myFoo); 比如123456789101112131415161718192021222324252627282930import java.io.*; public class Box implements Serializable &#123; private int width; private int height; public void setWidth(int width)&#123; this.width = width; &#125; public void setHeight(int height)&#123; this.height = height; &#125; public static void main(String[] args)&#123; Box myBox = new Box(); myBox.setWidth(50); myBox.setHeight(30); try&#123; FileOutputStream fs = new FileOutputStream(&quot;foo.ser&quot;); ObjectOutputStream os = new ObjectOutputStream(fs); os.writeObject(myBox); os.close(); &#125;catch(Exception ex)&#123; ex.printStackTrace(); &#125; &#125; &#125; 相关注意事项1.当一个父类实现序列化，子类自动实现序列化，不需要显式实现Serializable接口；2.当一个对象的实例变量引用其他对象，序列化该对象时也把引用对象进行序列化；3.并非所有的对象都可以序列化。 总结就像你寄一箱饼干，因为体积太大，就全压成粉末紧紧地一包寄出去，这就是序列化的作用。只不过JAVA的序列化是可以完全还原的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[接口详解]]></title>
      <url>%2F2017%2F04%2F08%2F%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[接口详解 接口的概念类是一种具体实现体，而接口定义了一种规范，接口定义了某一批类所需要遵守的规范，接口不关心这些类的内部状态数据，也不关心类里的实现细节，只规定了这批类里必须提供某些方法。 可见，接口不提供任何实现方法。接口体现的是规范和实现分离的哲学。 接口定义注意事项 接口可以有多个直接父接口，支持多继承，但接口只能继承接口，不能继承类。 接口里不能包含构造器和初始化块定义。接口里可以包含成员变量（只能是静态常量）、方法（只能是抽象实例方法、类方法或默认方法）、内部类（包括内部接口、枚举）。 系统自动为接口里定义的成员变量增加public static final修饰符。 接口的使用接口的主要用途： 定义变量，也可用于强制类型转换。 调用接口中定义的常量 被其他类实现 接口和抽象类设计目的上的差别:接口体现的是一种规范，是多个模块间的耦合标准。接口类似于整个系统的“总纲”，因此，接口不应该经常被改写。抽象类则不一样，体现的是一种模板式设计。可以在后期更加完善。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[疯狂JAVA摘句]]></title>
      <url>%2F2017%2F03%2F20%2F%E7%96%AF%E7%8B%82JAVA%E6%91%98%E5%8F%A5%2F</url>
      <content type="text"><![CDATA[真正优秀的程序员当然可以使用IDE工具，但真正的程序员，即使使用vi、记事本也一样可以完成非常优秀的项目。正确对待IDE工具的态度是：可以使用IDE工具，但绝不可依赖IDE工具。学习阶段，前期不要使用IDE工具；开发阶段，使用IDE工具。真正技术掌握了，无论用什么IDE工具都得心应手。 对于IDE工具，业内有一个说法：IDE工具会加快高手的开发效率，但会使初学者更白痴。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Git命令实践]]></title>
      <url>%2F2017%2F03%2F19%2FGit%E5%91%BD%E4%BB%A4%E5%AE%9E%E8%B7%B5%2F</url>
      <content type="text"><![CDATA[Git入门标签（空格分隔）： 基本操作 git init – 初始化仓库使用Git仓库管理必须先初始化仓库。1234mkdir Git-firstcd Git-firstgit initInitialized empty Git repository in /Volumes/CODE/Git-tutorial/.git/ git status – 查看仓库状态git status查看仓库状态。123456$git statusOn branch masterInitial commitnothing to commit (create/copy files and use &quot;git add&quot; to track) 提示显示正处在: master分支处，且没有任何文件。创建第一个文件READEME.md12345678910111213$touch READE.md$Git-tutorial ZH$ git statusOn branch masterInitial commitUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) READE.mdnothing added to commit but untracked files present (use &quot;git add&quot; to track) 可以在Untracked files：中看到这个文件，且并没有添加到暂存区 git add – 向暂存区添加文件要想让文件成为Git仓库管理的对象，需要用git add命令添加到暂存区。暂存区是提交前的一个临时区域。12345678910$ git add README.md $ git statusOn branch masterInitial commitChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: README.md 可以看到READEME.md文件显示在Changes to be committed中了。 git commit – 保存仓库的历史记录git commit将当前暂存区中的文件实际保存到仓库的历史记录中。 记叙一行提交信息1234$ git commit -m &quot;First Commit&quot;[master (root-commit) a6ce11d] First Commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.md -m参数后的“First Commit”成为提交信息，是对这个提交的描述。 记叙详细提交信息]]></content>
    </entry>

    
  
  
</search>
